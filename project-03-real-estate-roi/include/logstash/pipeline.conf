# ============================================
# Logstash Pipeline Configuration
# Collects logs from Docker containers
# ============================================

input {
  # Collect logs from Docker containers via TCP
  tcp {
    port => 5044
    codec => json
  }

  # Alternative: Read from Docker log files
  file {
    path => "/var/log/containers/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => json
  }
}

filter {
  # Parse timestamp
  date {
    match => ["time", "ISO8601"]
    target => "@timestamp"
  }

  # Add container metadata
  if [container_name] {
    mutate {
      add_field => { "service" => "%{container_name}" }
    }
  }

  # Parse Airflow logs
  if [service] =~ /airflow/ {
    grok {
      match => { "message" => "\[%{TIMESTAMP_ISO8601:log_time}\] \{%{DATA:dag_id}:%{DATA:task_id}\} %{LOGLEVEL:level} - %{GREEDYDATA:log_message}" }
    }
  }

  # Parse Kafka logs
  if [service] =~ /kafka/ {
    grok {
      match => { "message" => "\[%{TIMESTAMP_ISO8601:log_time}\] %{LOGLEVEL:level} %{GREEDYDATA:log_message}" }
    }
  }

  # Tag error messages
  if [level] in ["ERROR", "FATAL", "CRITICAL"] {
    mutate {
      add_tag => ["error"]
    }
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "real-estate-logs-%{+YYYY.MM.dd}"
  }

  # Also output to stdout for debugging
  stdout {
    codec => rubydebug
  }
}
